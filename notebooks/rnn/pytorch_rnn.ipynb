{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = 'arn:aws:iam::142577830533:role/SageMakerRole'#get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download training and test data\n",
    "We use raw data from the wikitext-2 dataset:\n",
    "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn\n",
      "data_dir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn/data/wikitext-2\n"
     ]
    }
   ],
   "source": [
    "# script to download dataset\n",
    "import os\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "data_dir = os.path.join(workbookDir, 'data', 'wikitext-2')\n",
    "print('data_dir: ' + data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the data\n",
    "We use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-142577830533/data/DEMO-pytorch-rnn\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-pytorch-rnn')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on github.com/pytorch/examples/blob/master/word_language_model\r\n",
      "import time\r\n",
      "import logging\r\n",
      "import math\r\n",
      "import os\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "\r\n",
      "import data\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "print(\"user script!!!\")\r\n",
      "print(\"user script!!!\")\r\n",
      "print(\"user script!!!\")\r\n",
      "print(\"user script!!!\")\r\n",
      "print(\"user script!!!\")\r\n",
      "print(\"user script!!!\")\r\n",
      "\r\n",
      "\r\n",
      "class RNNModel(nn.Module):\r\n",
      "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\r\n",
      "\r\n",
      "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\r\n",
      "        super(RNNModel, self).__init__()\r\n",
      "        self.drop = nn.Dropout(dropout)\r\n",
      "        self.encoder = nn.Embedding(ntoken, ninp)\r\n",
      "        if rnn_type in ['LSTM', 'GRU']:\r\n",
      "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\r\n",
      "        else:\r\n",
      "            try:\r\n",
      "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\r\n",
      "            except KeyError:\r\n",
      "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\r\n",
      "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\r\n",
      "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\r\n",
      "        self.decoder = nn.Linear(nhid, ntoken)\r\n",
      "\r\n",
      "        # Optionally tie weights as in:\r\n",
      "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\r\n",
      "        # https://arxiv.org/abs/1608.05859\r\n",
      "        # and\r\n",
      "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\r\n",
      "        # https://arxiv.org/abs/1611.01462\r\n",
      "        if tie_weights:\r\n",
      "            if nhid != ninp:\r\n",
      "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\r\n",
      "            self.decoder.weight = self.encoder.weight\r\n",
      "\r\n",
      "        self.init_weights()\r\n",
      "\r\n",
      "        self.rnn_type = rnn_type\r\n",
      "        self.nhid = nhid\r\n",
      "        self.nlayers = nlayers\r\n",
      "\r\n",
      "    def init_weights(self):\r\n",
      "        initrange = 0.1\r\n",
      "        self.encoder.weight.data.uniform_(-initrange, initrange)\r\n",
      "        self.decoder.bias.data.zero_()\r\n",
      "        self.decoder.weight.data.uniform_(-initrange, initrange)\r\n",
      "\r\n",
      "    def forward(self, input, hidden):\r\n",
      "        emb = self.drop(self.encoder(input))\r\n",
      "        output, hidden = self.rnn(emb, hidden)\r\n",
      "        output = self.drop(output)\r\n",
      "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\r\n",
      "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\r\n",
      "\r\n",
      "    def init_hidden(self, bsz):\r\n",
      "        weight = next(self.parameters())\r\n",
      "        if self.rnn_type == 'LSTM':\r\n",
      "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\r\n",
      "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\r\n",
      "        else:\r\n",
      "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\r\n",
      "\r\n",
      "\r\n",
      "# Starting from sequential data, batchify arranges the dataset into columns.\r\n",
      "# For instance, with the alphabet as the sequence and batch size 4, we'd get\r\n",
      "# ┌ a g m s ┐\r\n",
      "# │ b h n t │\r\n",
      "# │ c i o u │\r\n",
      "# │ d j p v │\r\n",
      "# │ e k q w │\r\n",
      "# └ f l r x ┘.\r\n",
      "# These columns are treated as independent by the model, which means that the\r\n",
      "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\r\n",
      "# batch processing.\r\n",
      "def batchify(data, bsz, device):\r\n",
      "    # Work out how cleanly we can divide the dataset into bsz parts.\r\n",
      "    nbatch = data.size(0) // bsz\r\n",
      "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\r\n",
      "    data = data.narrow(0, 0, nbatch * bsz)\r\n",
      "    # Evenly divide the data across the bsz batches.\r\n",
      "    data = data.view(bsz, -1).t().contiguous()\r\n",
      "    return data.to(device)\r\n",
      "\r\n",
      "###############################################################################\r\n",
      "# Training code\r\n",
      "###############################################################################\r\n",
      "\r\n",
      "\r\n",
      "def repackage_hidden(h):\r\n",
      "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\r\n",
      "    if isinstance(h, torch.Tensor):\r\n",
      "        return h.detach()\r\n",
      "    else:\r\n",
      "        return tuple(repackage_hidden(v) for v in h)\r\n",
      "\r\n",
      "\r\n",
      "# get_batch subdivides the source data into chunks of length bptt.\r\n",
      "# If source is equal to the example output of the batchify function, with\r\n",
      "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\r\n",
      "# ┌ a g m s ┐ ┌ b h n t ┐\r\n",
      "# └ b h n t ┘ └ c i o u ┘\r\n",
      "# Note that despite the name of the function, the subdivison of data is not\r\n",
      "# done along the batch dimension (i.e. dimension 1), since that was handled\r\n",
      "# by the batchify function. The chunks are along dimension 0, corresponding\r\n",
      "# to the seq_len dimension in the LSTM.\r\n",
      "def get_batch(source, i, bptt):\r\n",
      "    seq_len = min(bptt, len(source) - 1 - i)\r\n",
      "    data = source[i:i+seq_len]\r\n",
      "    target = source[i+1:i+1+seq_len].view(-1)\r\n",
      "    return data, target\r\n",
      "\r\n",
      "\r\n",
      "def evaluate(data_source, model, corpus, eval_batch_size, criterion, bptt):\r\n",
      "    # Turn on evaluation mode which disables dropout.\r\n",
      "    model.eval()\r\n",
      "    total_loss = 0.\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(eval_batch_size)\r\n",
      "    with torch.no_grad():\r\n",
      "        for i in range(0, data_source.size(0) - 1, bptt):\r\n",
      "            data, targets = get_batch(data_source, i, bptt)\r\n",
      "            output, hidden = model(data, hidden)\r\n",
      "            output_flat = output.view(-1, ntokens)\r\n",
      "            total_loss += len(data) * criterion(output_flat, targets).item()\r\n",
      "            hidden = repackage_hidden(hidden)\r\n",
      "    return total_loss / len(data_source)\r\n",
      "\r\n",
      "\r\n",
      "def train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval):\r\n",
      "    # Turn on training mode which enables dropout.\r\n",
      "    model.train()\r\n",
      "    total_loss = 0.\r\n",
      "    start_time = time.time()\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(batch_size)\r\n",
      "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\r\n",
      "        data, targets = get_batch(train_data, i, bptt)\r\n",
      "        # Starting each batch, we detach the hidden state from how it was previously produced.\r\n",
      "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\r\n",
      "        hidden = repackage_hidden(hidden)\r\n",
      "        model.zero_grad()\r\n",
      "        output, hidden = model(data, hidden)\r\n",
      "        loss = criterion(output.view(-1, ntokens), targets)\r\n",
      "        loss.backward()\r\n",
      "\r\n",
      "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
      "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
      "        for p in model.parameters():\r\n",
      "            p.data.add_(-lr, p.grad.data)\r\n",
      "\r\n",
      "        total_loss += loss.item()\r\n",
      "\r\n",
      "        if batch % log_interval == 0 and batch > 0:\r\n",
      "            cur_loss = total_loss / log_interval\r\n",
      "            elapsed = time.time() - start_time\r\n",
      "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\r\n",
      "                  'loss {:5.2f} | ppl {:8.2f}'.format(\r\n",
      "                epoch, batch, len(train_data) // bptt, lr,\r\n",
      "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\r\n",
      "            total_loss = 0\r\n",
      "            start_time = time.time()\r\n",
      "\r\n",
      "\r\n",
      "def train(channel_input_dirs, model_dir, host_rank, master_addr, master_port, hyperparameters={}):\r\n",
      "    logger.info('def train')\r\n",
      "    data_dir = channel_input_dirs['wikitext-2']\r\n",
      "    logger.debug('data_dir: {}'.format(data_dir))\r\n",
      "    model_path = os.path.join(model_dir, 'model')\r\n",
      "    model, emsize, nhid, nlayers, lr, clip, epochs, \\\r\n",
      "        batch_size, bptt, dropout, tied, seed, log_interval = _load_hyperparameters(hyperparameters)\r\n",
      "    logger.debug('model - {}, emsize - {}, nhid - {}, nlayers - {}, lr - {}, clip - {}, epochs - {}, batch_size - {}, bptt - {}, dropout - {}, tied - {}, seed - {}, log_interval - {}'.format(\r\n",
      "        model, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\r\n",
      "    ))\r\n",
      "\r\n",
      "    # Set the random seed manually for reproducibility.\r\n",
      "    torch.manual_seed(seed)\r\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "    logger.debug('Device: {}'.format(device))\r\n",
      "    # Load data\r\n",
      "    corpus = data.Corpus(data_dir)\r\n",
      "    logger.debug('corpus: {}'.format(corpus))\r\n",
      "\r\n",
      "    # Batchify\r\n",
      "    eval_batch_size = 10\r\n",
      "    train_data = batchify(corpus.train, batch_size, device)\r\n",
      "    val_data = batchify(corpus.valid, eval_batch_size, device)\r\n",
      "    test_data = batchify(corpus.test, eval_batch_size, device)\r\n",
      "\r\n",
      "    # Build the model\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    model = RNNModel(model, ntokens, emsize, nhid, nlayers, dropout, tied).to(\r\n",
      "        device)\r\n",
      "\r\n",
      "    criterion = nn.CrossEntropyLoss()\r\n",
      "\r\n",
      "    # Loop over epochs.\r\n",
      "    lr = lr\r\n",
      "    best_val_loss = None\r\n",
      "\r\n",
      "    for epoch in range(1, epochs + 1):\r\n",
      "        epoch_start_time = time.time()\r\n",
      "        train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval)\r\n",
      "        val_loss = evaluate(val_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "        print('-' * 89)\r\n",
      "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\r\n",
      "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\r\n",
      "                                         val_loss, math.exp(val_loss)))\r\n",
      "        print('-' * 89)\r\n",
      "        # Save the model if the validation loss is the best we've seen so far.\r\n",
      "        if not best_val_loss or val_loss < best_val_loss:\r\n",
      "            with open(model_path, 'wb') as f:\r\n",
      "                torch.save(model.state_dict(), f)\r\n",
      "            best_val_loss = val_loss\r\n",
      "        else:\r\n",
      "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\r\n",
      "            lr /= 4.0\r\n",
      "\r\n",
      "    # Load the best saved model.\r\n",
      "    with open(model_path, 'rb') as f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "        # after load the rnn params are not a continuous chunk of memory\r\n",
      "        # this makes them a continuous chunk, and will speed up forward pass\r\n",
      "        model.rnn.flatten_parameters()\r\n",
      "\r\n",
      "    # Run on test data.\r\n",
      "    test_loss = evaluate(test_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "    print('=' * 89)\r\n",
      "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\r\n",
      "        test_loss, math.exp(test_loss)))\r\n",
      "    print('=' * 89)\r\n",
      "\r\n",
      "\r\n",
      "def _load_hyperparameters(hyperparameters):\r\n",
      "    logger.info(\"Load hyperparameters\")\r\n",
      "    # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\r\n",
      "    model = hyperparameters.get('model', 'LSTM')\r\n",
      "    logger.debug('model: {}'.format(model))\r\n",
      "    # size of word embeddings\r\n",
      "    emsize = hyperparameters.get('emsize', 200)\r\n",
      "    logger.debug('emsize: {}'.format(emsize))\r\n",
      "    # number of hidden units per layer\r\n",
      "    nhid = hyperparameters.get('nhid', 200)\r\n",
      "    logger.debug('nhid: {}'.format(nhid))\r\n",
      "    # number of layers\r\n",
      "    nlayers = hyperparameters.get('nlayers', 2)\r\n",
      "    logger.debug('nlayers: {}'.format(nlayers))\r\n",
      "    # initial learning rate\r\n",
      "    lr = hyperparameters.get('lr', 20)\r\n",
      "    logger.debug('lr: {}'.format(lr))\r\n",
      "    # gradient clipping\r\n",
      "    clip = hyperparameters.get('clip', 0.25)\r\n",
      "    logger.debug('clip: {}'.format(clip))\r\n",
      "    # upper epoch limit\r\n",
      "    epochs = hyperparameters.get('epochs', 40)\r\n",
      "    logger.debug('epochs: {}'.format(epochs))\r\n",
      "    # batch size\r\n",
      "    batch_size = hyperparameters.get('batch_size', 20)\r\n",
      "    logger.debug('batch_size: {}'.format(batch_size))\r\n",
      "    # sequence length\r\n",
      "    bptt = hyperparameters.get('bptt', 35)\r\n",
      "    logger.debug('bptt: {}'.format(bptt))\r\n",
      "    # dropout applied to layers (0 = no dropout)\r\n",
      "    dropout = hyperparameters.get('dropout', 0.2)\r\n",
      "    logger.debug('dropout: {}'.format(dropout))\r\n",
      "    # tie the word embedding and softmax weights\r\n",
      "    tied = hyperparameters.get('tied', False)\r\n",
      "    logger.debug('tied: {}'.format(tied))\r\n",
      "    # random seed\r\n",
      "    seed = hyperparameters.get('seed', 1111)\r\n",
      "    logger.debug('seed: {}'.format(seed))\r\n",
      "    # report interval\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "    logger.debug('model - {}, emsize - {}, nhid - {}, nlayers - {}, lr - {}, clip - {}, epochs - {}, batch_size - {}, bptt - {}, dropout - {}, tied - {}, seed - {}, log_interval - {}'.format(\r\n",
      "        model, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\r\n",
      "    ))\r\n",
      "    return model, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\r\n",
      "\r\n",
      "\r\n",
      "'''\r\n",
      "if __name__ == '__main__':\r\n",
      "    rnn_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\r\n",
      "    data_dir = os.path.join(rnn_dir,  'data', 'wikitext-2')\r\n",
      "    train({'training': data_dir}, data_dir)\r\n",
      "'''\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'source/rnn.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training script on SageMaker\n",
    "The PyTorch class allows us to run our training function as a distributed training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p2.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"rnn.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.8xlarge',\n",
    "                    source_dir='source',\n",
    "                    hyperparameters={'batch_size': 30, 'epochs': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-04-30-20-15-06-933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................\n",
      "\u001b[31m2018-04-30 20:20:10,345 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,345 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,408 INFO - container_support.app - started training: {'train_fn': <function train at 0x7fea4538b488>}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-142577830533/sagemaker-pytorch-2018-04-30-20-15-06-933/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,555 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,658 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,711 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,727 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,798 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,884 INFO - pytorch_container.training - Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 INFO - rnn - def train\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - data_dir: /opt/ml/input/data/wikitext-2\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 INFO - rnn - Load hyperparameters\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - model: LSTM\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - emsize: 200\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - nhid: 200\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - nlayers: 2\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - lr: 20\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - clip: 0.25\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - epochs: 50\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - batch_size: 30\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - bptt: 35\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - dropout: 0.2\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - tied: False\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - seed: 1111\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - model - LSTM, emsize - 200, nhid - 200, nlayers - 2, lr - 20, clip - 0.25, epochs - 50, batch_size - 30, bptt - 35, dropout - 0.2, tied - False, seed - 1111, log_interval - 200\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:10,885 DEBUG - rnn - model - LSTM, emsize - 200, nhid - 200, nlayers - 2, lr - 20, clip - 0.25, epochs - 50, batch_size - 30, bptt - 35, dropout - 0.2, tied - False, seed - 1111, log_interval - 200\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:11,187 DEBUG - rnn - Device: cuda\u001b[0m\n",
      "\u001b[31m2018-04-30 20:20:15,699 DEBUG - rnn - corpus: <data.Corpus object at 0x7fea44d298d0>\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/  233 batches | lr 20.00 | ms/batch 38.59 | loss  7.09 | ppl  1194.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 12.60s | valid loss  6.77 | valid ppl   872.93\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/  233 batches | lr 20.00 | ms/batch 38.35 | loss  6.27 | ppl   529.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 12.57s | valid loss  6.39 | valid ppl   593.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/  233 batches | lr 20.00 | ms/batch 38.40 | loss  5.89 | ppl   359.64\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 12.56s | valid loss  6.05 | valid ppl   424.84\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/  233 batches | lr 20.00 | ms/batch 38.42 | loss  5.59 | ppl   266.75\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 12.59s | valid loss  5.92 | valid ppl   371.71\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/  233 batches | lr 20.00 | ms/batch 38.44 | loss  5.34 | ppl   209.22\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 12.60s | valid loss  5.85 | valid ppl   347.89\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/  233 batches | lr 20.00 | ms/batch 38.52 | loss  5.14 | ppl   170.44\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 12.60s | valid loss  5.80 | valid ppl   328.94\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   7 |   200/  233 batches | lr 20.00 | ms/batch 38.59 | loss  4.95 | ppl   141.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   7 | time: 12.64s | valid loss  5.83 | valid ppl   342.04\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   8 |   200/  233 batches | lr 5.00 | ms/batch 38.43 | loss  4.65 | ppl   104.88\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   8 | time: 12.61s | valid loss  5.73 | valid ppl   307.29\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   9 |   200/  233 batches | lr 5.00 | ms/batch 38.71 | loss  4.55 | ppl    94.82\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   9 | time: 12.66s | valid loss  5.73 | valid ppl   306.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  10 |   200/  233 batches | lr 5.00 | ms/batch 38.95 | loss  4.48 | ppl    88.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  10 | time: 12.72s | valid loss  5.73 | valid ppl   307.14\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  11 |   200/  233 batches | lr 1.25 | ms/batch 38.92 | loss  4.40 | ppl    81.79\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  11 | time: 12.73s | valid loss  5.72 | valid ppl   304.65\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  12 |   200/  233 batches | lr 1.25 | ms/batch 38.98 | loss  4.37 | ppl    79.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  12 | time: 12.71s | valid loss  5.72 | valid ppl   304.89\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  13 |   200/  233 batches | lr 0.31 | ms/batch 38.97 | loss  4.35 | ppl    77.80\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  13 | time: 12.74s | valid loss  5.71 | valid ppl   303.35\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  14 |   200/  233 batches | lr 0.31 | ms/batch 38.98 | loss  4.34 | ppl    76.90\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  14 | time: 12.74s | valid loss  5.71 | valid ppl   303.21\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  15 |   200/  233 batches | lr 0.31 | ms/batch 39.00 | loss  4.34 | ppl    76.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  15 | time: 12.72s | valid loss  5.71 | valid ppl   303.35\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  16 |   200/  233 batches | lr 0.08 | ms/batch 39.00 | loss  4.33 | ppl    76.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  16 | time: 12.75s | valid loss  5.71 | valid ppl   303.02\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  17 |   200/  233 batches | lr 0.08 | ms/batch 38.99 | loss  4.33 | ppl    75.91\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  17 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  18 |   200/  233 batches | lr 0.08 | ms/batch 38.99 | loss  4.33 | ppl    75.77\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  18 | time: 12.72s | valid loss  5.71 | valid ppl   302.95\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  19 |   200/  233 batches | lr 0.08 | ms/batch 39.02 | loss  4.33 | ppl    75.73\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  19 | time: 12.75s | valid loss  5.71 | valid ppl   303.02\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  20 |   200/  233 batches | lr 0.02 | ms/batch 39.00 | loss  4.33 | ppl    75.68\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  20 | time: 12.74s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  21 |   200/  233 batches | lr 0.00 | ms/batch 39.00 | loss  4.32 | ppl    75.51\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  21 | time: 12.73s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  22 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.33 | ppl    75.69\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  22 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  23 |   200/  233 batches | lr 0.00 | ms/batch 39.00 | loss  4.33 | ppl    75.65\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  23 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  24 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.32 | ppl    75.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  24 | time: 12.73s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  25 |   200/  233 batches | lr 0.00 | ms/batch 39.03 | loss  4.33 | ppl    75.61\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  25 | time: 12.74s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  26 |   200/  233 batches | lr 0.00 | ms/batch 38.99 | loss  4.32 | ppl    75.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  26 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  27 |   200/  233 batches | lr 0.00 | ms/batch 39.02 | loss  4.32 | ppl    75.44\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  27 | time: 12.73s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  28 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.33 | ppl    75.66\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  28 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  29 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  29 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  30 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.50\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  30 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  31 |   200/  233 batches | lr 0.00 | ms/batch 39.09 | loss  4.32 | ppl    75.39\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  31 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  32 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.51\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  32 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  33 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.32 | ppl    75.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  33 | time: 12.73s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  34 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.52\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  34 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  35 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  35 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch  36 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.32 | ppl    75.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  36 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  37 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  37 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  38 |   200/  233 batches | lr 0.00 | ms/batch 39.13 | loss  4.32 | ppl    75.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  38 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  39 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  39 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  40 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  40 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  41 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.29\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  41 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  42 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  42 | time: 12.73s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  43 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.32 | ppl    75.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  43 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  44 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.32 | ppl    75.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  44 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  45 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.44\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  45 | time: 12.75s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  46 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  46 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  47 |   200/  233 batches | lr 0.00 | ms/batch 39.10 | loss  4.32 | ppl    75.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  47 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  48 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.48\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  48 | time: 12.74s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  49 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  49 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  50 |   200/  233 batches | lr 0.00 | ms/batch 39.02 | loss  4.32 | ppl    75.38\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  50 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  4.13 | test ppl    61.97\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 799\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'wikitext-2': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the training function\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a train function. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.fw_utils import create_image_uri, framework_name_from_image, framework_version_from_tag\n",
    "DATA_DIR = 'data_dir'\n",
    "BUCKET_NAME = 'mybucket'\n",
    "ROLE = 'Sagemaker'\n",
    "REGION = 'us-west-2'\n",
    "SCRIPT_PATH = 'script.py'\n",
    "image_uri = create_image_uri('mars-south-3', 'mlfw', 'ml.c4.large', '1.0rc', 'py2')\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
