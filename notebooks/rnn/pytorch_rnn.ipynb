{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "This example trains a multi-layer RNN (Elman, GRU, or LSTM) model on a language modeling task. By default, the training script uses the Wikitext-2 dataset. We will train a model on SageMaker, deploy it, and then use deployed model to generate new text.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p3.2xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/<notebook_specific_prefix_here>' # notebook author to input the proper prefix\n",
    "\n",
    "import sagemaker\n",
    "role = 'arn:aws:iam::142577830533:role/SageMakerRole'#sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import the Python libraries we'll need and start sagemaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "As mentioned above we are going to use [the wikitext-2 raw data](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn\n",
      "data_dir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn/data/training\n"
     ]
    }
   ],
   "source": [
    "# script to download dataset\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "import os\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "data_dir = os.path.join(workbookDir, 'data', 'training')\n",
    "print('data_dir: ' + data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-142577830533/data/DEMO-pytorch-rnn\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-pytorch-rnn')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "We need to provide a training script that can run on the SageMaker platform. When SageMaker calls your `train()` function, it will pass in arguments that describe the training environment. Check the script below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on github.com/pytorch/examples/blob/master/word_language_model\r\n",
      "import time\r\n",
      "import logging\r\n",
      "import math\r\n",
      "import os\r\n",
      "from shutil import copy\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "\r\n",
      "import data\r\n",
      "from rnn import RNNModel\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "\r\n",
      "\r\n",
      "# Starting from sequential data, batchify arranges the dataset into columns.\r\n",
      "# For instance, with the alphabet as the sequence and batch size 4, we'd get\r\n",
      "# ┌ a g m s ┐\r\n",
      "# │ b h n t │\r\n",
      "# │ c i o u │\r\n",
      "# │ d j p v │\r\n",
      "# │ e k q w │\r\n",
      "# └ f l r x ┘.\r\n",
      "# These columns are treated as independent by the model, which means that the\r\n",
      "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\r\n",
      "# batch processing.\r\n",
      "def batchify(data, bsz, device):\r\n",
      "    # Work out how cleanly we can divide the dataset into bsz parts.\r\n",
      "    nbatch = data.size(0) // bsz\r\n",
      "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\r\n",
      "    data = data.narrow(0, 0, nbatch * bsz)\r\n",
      "    # Evenly divide the data across the bsz batches.\r\n",
      "    data = data.view(bsz, -1).t().contiguous()\r\n",
      "    return data.to(device)\r\n",
      "\r\n",
      "###############################################################################\r\n",
      "# Training code\r\n",
      "###############################################################################\r\n",
      "\r\n",
      "\r\n",
      "def repackage_hidden(h):\r\n",
      "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\r\n",
      "    if isinstance(h, torch.Tensor):\r\n",
      "        return h.detach()\r\n",
      "    else:\r\n",
      "        return tuple(repackage_hidden(v) for v in h)\r\n",
      "\r\n",
      "\r\n",
      "# get_batch subdivides the source data into chunks of length bptt.\r\n",
      "# If source is equal to the example output of the batchify function, with\r\n",
      "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\r\n",
      "# ┌ a g m s ┐ ┌ b h n t ┐\r\n",
      "# └ b h n t ┘ └ c i o u ┘\r\n",
      "# Note that despite the name of the function, the subdivison of data is not\r\n",
      "# done along the batch dimension (i.e. dimension 1), since that was handled\r\n",
      "# by the batchify function. The chunks are along dimension 0, corresponding\r\n",
      "# to the seq_len dimension in the LSTM.\r\n",
      "def get_batch(source, i, bptt):\r\n",
      "    seq_len = min(bptt, len(source) - 1 - i)\r\n",
      "    data = source[i:i+seq_len]\r\n",
      "    target = source[i+1:i+1+seq_len].view(-1)\r\n",
      "    return data, target\r\n",
      "\r\n",
      "\r\n",
      "def evaluate(data_source, model, corpus, eval_batch_size, criterion, bptt):\r\n",
      "    # Turn on evaluation mode which disables dropout.\r\n",
      "    model.eval()\r\n",
      "    total_loss = 0.\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(eval_batch_size)\r\n",
      "    with torch.no_grad():\r\n",
      "        for i in range(0, data_source.size(0) - 1, bptt):\r\n",
      "            data, targets = get_batch(data_source, i, bptt)\r\n",
      "            output, hidden = model(data, hidden)\r\n",
      "            output_flat = output.view(-1, ntokens)\r\n",
      "            total_loss += len(data) * criterion(output_flat, targets).item()\r\n",
      "            hidden = repackage_hidden(hidden)\r\n",
      "    return total_loss / len(data_source)\r\n",
      "\r\n",
      "\r\n",
      "def train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval):\r\n",
      "    # Turn on training mode which enables dropout.\r\n",
      "    model.train()\r\n",
      "    total_loss = 0.\r\n",
      "    start_time = time.time()\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(batch_size)\r\n",
      "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\r\n",
      "        data, targets = get_batch(train_data, i, bptt)\r\n",
      "        # Starting each batch, we detach the hidden state from how it was previously produced.\r\n",
      "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\r\n",
      "        hidden = repackage_hidden(hidden)\r\n",
      "        model.zero_grad()\r\n",
      "        output, hidden = model(data, hidden)\r\n",
      "        loss = criterion(output.view(-1, ntokens), targets)\r\n",
      "        loss.backward()\r\n",
      "\r\n",
      "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
      "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
      "        for p in model.parameters():\r\n",
      "            p.data.add_(-lr, p.grad.data)\r\n",
      "\r\n",
      "        total_loss += loss.item()\r",
      "\r\n",
      "\r\n",
      "        if batch % log_interval == 0 and batch > 0:\r\n",
      "            cur_loss = total_loss / log_interval\r\n",
      "            elapsed = time.time() - start_time\r\n",
      "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\r\n",
      "                  'loss {:5.2f} | ppl {:8.2f}'.format(\r\n",
      "                epoch, batch, len(train_data) // bptt, lr,\r\n",
      "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\r\n",
      "            total_loss = 0\r\n",
      "            start_time = time.time()\r\n",
      "\r\n",
      "\r\n",
      "# TODO(nadiaya): remove host_rank, master_addr, master_port arguments after container_support exists\r\n",
      "def train(channel_input_dirs, model_dir, host_rank, master_addr, master_port, hyperparameters={}):\r\n",
      "    logger.info('Starting training.')\r\n",
      "    data_dir = channel_input_dirs['training']\r\n",
      "    model_path = os.path.join(model_dir, 'model.pth')\r\n",
      "    model_info_path = os.path.join(model_dir, 'model_info.pth')\r\n",
      "    model_state_path = os.path.join(model_dir, 'model_state.txt')\r\n",
      "    rnn_type, emsize, nhid, nlayers, lr, clip, epochs, \\\r\n",
      "        batch_size, bptt, dropout, tied, seed, log_interval = _load_hyperparameters(hyperparameters)\r\n",
      "\r\n",
      "    # Set the random seed manually for reproducibility.\r\n",
      "    torch.manual_seed(seed)\r\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "    logger.debug('Device: {}'.format(device))\r\n",
      "\r\n",
      "    # Load data\r\n",
      "    corpus = data.Corpus(data_dir)\r\n",
      "\r\n",
      "    # Save the data into model dir to be used with the model later\r\n",
      "    for file_name in os.listdir(data_dir):\r\n",
      "        full_file_name = os.path.join(data_dir, file_name)\r\n",
      "        if os.path.isfile(full_file_name):\r\n",
      "            copy(full_file_name, model_dir)\r\n",
      "\r\n",
      "    # Batchify\r\n",
      "    eval_batch_size = 10\r\n",
      "    train_data = batchify(corpus.train, batch_size, device)\r\n",
      "    val_data = batchify(corpus.valid, eval_batch_size, device)\r\n",
      "    test_data = batchify(corpus.test, eval_batch_size, device)\r\n",
      "\r\n",
      "    # Build the model\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    # Save arguments used to create model for restoring the model later\r\n",
      "    with open(model_info_path, 'wb') as f:\r\n",
      "        model_info = {\r\n",
      "            'rnn_type': rnn_type,\r\n",
      "            'ntoken': ntokens,\r\n",
      "            'ninp': emsize,\r\n",
      "            'nhid': nhid,\r\n",
      "            'nlayers': nlayers,\r\n",
      "            'dropout': dropout,\r\n",
      "            'tie_weights': tied\r\n",
      "        }\r\n",
      "        torch.save(model_info, f)\r\n",
      "    model = RNNModel(rnn_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(\r\n",
      "        device)\r\n",
      "\r\n",
      "    criterion = nn.CrossEntropyLoss()\r\n",
      "\r\n",
      "    # Loop over epochs.\r\n",
      "    lr = lr\r\n",
      "    best_state = None\r\n",
      "\r\n",
      "    for epoch in range(1, epochs + 1):\r\n",
      "        epoch_start_time = time.time()\r\n",
      "        train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval)\r\n",
      "        val_loss = evaluate(val_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "        print('-' * 89)\r\n",
      "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\r\n",
      "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\r\n",
      "                                         val_loss, math.exp(val_loss)))\r\n",
      "        print('-' * 89)\r\n",
      "        # Save the model if the validation loss is the best we've seen so far.\r\n",
      "        if not best_state or val_loss < best_state['val_loss']:\r\n",
      "            best_state = {\r\n",
      "                'epoch': epoch,\r\n",
      "                'lr': lr,\r\n",
      "                'val_loss': val_loss,\r\n",
      "                'val_ppl': math.exp(val_loss),\r\n",
      "            }\r\n",
      "            logger.info('Saving the best model: {}'.format(best_state))\r\n",
      "            with open(model_path, 'wb') as f:\r\n",
      "                torch.save(model.state_dict(), f)\r\n",
      "            with open(model_state_path, 'w') as f:\r\n",
      "                f.write('epoch {:3d} | lr: {:5.2f} | valid loss {:5.2f} | '\r\n",
      "                        'valid ppl {:8.2f}'.format(epoch, lr, val_loss, math.exp(val_loss)))\r\n",
      "        else:\r\n",
      "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\r\n",
      "            lr /= 4.0\r\n",
      "\r\n",
      "    # Load the best saved model.\r\n",
      "    with open(model_path, 'rb') as f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "        # after load the rnn params are not a continuous chunk of memory\r\n",
      "        # this makes them a continuous chunk, and will speed up forward pass\r\n",
      "        model.rnn.flatten_parameters()\r\n",
      "\r\n",
      "    # Run on test data.\r\n",
      "    test_loss = evaluate(test_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "    print('=' * 89)\r\n",
      "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\r\n",
      "        test_loss, math.exp(test_loss)))\r\n",
      "    print('=' * 89)\r\n",
      "\r\n",
      "    # Return a cpu model so we can open it on any device\r\n",
      "    logger.info('Return the best model from: {}'.format(best_state))\r\n",
      "    return model.cpu()\r\n",
      "\r\n",
      "\r\n",
      "def _load_hyperparameters(hyperparameters):\r\n",
      "    logger.info(\"Load hyperparameters\")\r\n",
      "    # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\r\n",
      "    rnn_type = hyperparameters.get('rnn_type', 'LSTM')\r\n",
      "    logger.debug('model: {}'.format(rnn_type))\r\n",
      "    # size of word embeddings\r\n",
      "    emsize = hyperparameters.get('emsize', 200)\r\n",
      "    logger.debug('emsize: {}'.format(emsize))\r\n",
      "    # number of hidden units per layer\r\n",
      "    nhid = hyperparameters.get('nhid', 200)\r\n",
      "    logger.debug('nhid: {}'.format(nhid))\r\n",
      "    # number of layers\r\n",
      "    nlayers = hyperparameters.get('nlayers', 2)\r\n",
      "    logger.debug('nlayers: {}'.format(nlayers))\r\n",
      "    # initial learning rate\r\n",
      "    lr = hyperparameters.get('lr', 20)\r\n",
      "    logger.debug('lr: {}'.format(lr))\r\n",
      "    # gradient clipping\r\n",
      "    clip = hyperparameters.get('clip', 0.25)\r\n",
      "    logger.debug('clip: {}'.format(clip))\r\n",
      "    # upper epoch limit\r\n",
      "    epochs = hyperparameters.get('epochs', 40)\r\n",
      "    logger.debug('epochs: {}'.format(epochs))\r\n",
      "    # batch size\r\n",
      "    batch_size = hyperparameters.get('batch_size', 20)\r\n",
      "    logger.debug('batch_size: {}'.format(batch_size))\r\n",
      "    # sequence length\r\n",
      "    bptt = hyperparameters.get('bptt', 35)\r\n",
      "    logger.debug('bptt: {}'.format(bptt))\r\n",
      "    # dropout applied to layers (0 = no dropout)\r\n",
      "    dropout = hyperparameters.get('dropout', 0.2)\r\n",
      "    logger.debug('dropout: {}'.format(dropout))\r\n",
      "    # tie the word embedding and softmax weights\r\n",
      "    tied = hyperparameters.get('tied', False)\r\n",
      "    logger.debug('tied: {}'.format(tied))\r\n",
      "    # random seed\r\n",
      "    seed = hyperparameters.get('seed', 1111)\r\n",
      "    logger.debug('seed: {}'.format(seed))\r\n",
      "    # report interval\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "    logger.debug('log_interval: {}'.format(log_interval))\r\n",
      "    return rnn_type, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\r\n",
      "\r\n",
      "'''\r\n",
      "if __name__ == '__main__':\r\n",
      "    rnn_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\r\n",
      "    data_dir = os.path.join(rnn_dir,  'data', 'training')\r\n",
      "    model_dir = os.path.join(rnn_dir,  'output')\r\n",
      "    train({'training': data_dir}, model_dir, None, None, None)\r\n",
      "'''\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'source/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current example we also need to provide source directory since training script imports data and model classes from other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py   \u001b[34m__pycache__\u001b[m\u001b[m/  data.pyc      predict.py    rnn.pyc\r\n",
      "__init__.pyc  data.py       generate.py   rnn.py        train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p3.2xlarge instance. As you can see in this example you can also specify hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    source_dir='source',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'epochs': 15, \n",
    "                        'emsize':1500, \n",
    "                        'nhid':1500, \n",
    "                        'dropout':0.65, \n",
    "                        'tied': True\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-05-07-17-26-15-663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................\n",
      "\u001b[31m2018-05-07 17:30:10,532 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,532 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,543 INFO - container_support.app - started training: {'train_fn': <function train at 0x7f49b386d510>}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-142577830533/sagemaker-pytorch-2018-05-07-17-26-15-663/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,664 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,746 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,785 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,804 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,881 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:10,986 INFO - pytorch_container.training - Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 INFO - train - Starting training.\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 INFO - train - Load hyperparameters\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - model: GRU\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - emsize: 1500\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - nhid: 1500\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - nlayers: 2\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - lr: 20\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - clip: 0.25\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - epochs: 15\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - batch_size: 20\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - bptt: 35\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - dropout: 0.65\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,589 DEBUG - train - tied: True\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,590 DEBUG - train - seed: 1111\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,590 DEBUG - train - log_interval: 200\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:11,681 DEBUG - train - Device: cuda\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/  350 batches | lr 20.00 | ms/batch 37.26 | loss  8.75 | ppl  6286.92\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 17.49s | valid loss  7.21 | valid ppl  1359.25\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:30:42,988 INFO - train - Saving the best model: {'epoch': 1, 'val_ppl': 1359.2548704589794, 'lr': 20, 'val_loss': 7.214691939215475}\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/  350 batches | lr 20.00 | ms/batch 37.03 | loss  7.21 | ppl  1354.27\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 17.44s | valid loss  6.85 | valid ppl   944.66\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:31:00,874 INFO - train - Saving the best model: {'epoch': 2, 'val_ppl': 944.6594613891936, 'lr': 20, 'val_loss': 6.850824504230768}\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/  350 batches | lr 20.00 | ms/batch 37.99 | loss  6.61 | ppl   741.32\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 17.64s | valid loss  6.62 | valid ppl   750.87\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:31:18,864 INFO - train - Saving the best model: {'epoch': 3, 'val_ppl': 750.8741393086513, 'lr': 20, 'val_loss': 6.621238046918497}\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/  350 batches | lr 20.00 | ms/batch 37.50 | loss  6.23 | ppl   505.26\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 17.54s | valid loss  6.08 | valid ppl   436.52\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:31:36,746 INFO - train - Saving the best model: {'epoch': 4, 'val_ppl': 436.5159961425603, 'lr': 20, 'val_loss': 6.078825020982904}\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/  350 batches | lr 20.00 | ms/batch 37.84 | loss  5.93 | ppl   375.29\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 17.60s | valid loss  5.97 | valid ppl   391.74\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:31:54,679 INFO - train - Saving the best model: {'epoch': 5, 'val_ppl': 391.7361975531305, 'lr': 20, 'val_loss': 5.9705886478219}\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/  350 batches | lr 20.00 | ms/batch 37.28 | loss  5.70 | ppl   297.51\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 17.50s | valid loss  6.20 | valid ppl   491.42\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   7 |   200/  350 batches | lr 5.00 | ms/batch 37.05 | loss  5.21 | ppl   182.34\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   7 | time: 17.45s | valid loss  5.71 | valid ppl   301.42\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:32:30,330 INFO - train - Saving the best model: {'epoch': 7, 'val_ppl': 301.4214768256182, 'lr': 5.0, 'val_loss': 5.708509540540334}\u001b[0m\n",
      "\u001b[31m| epoch   8 |   200/  350 batches | lr 5.00 | ms/batch 37.91 | loss  5.05 | ppl   155.32\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   8 | time: 17.62s | valid loss  5.68 | valid ppl   293.39\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:32:48,288 INFO - train - Saving the best model: {'epoch': 8, 'val_ppl': 293.3856709854053, 'lr': 5.0, 'val_loss': 5.681488026702389}\u001b[0m\n",
      "\u001b[31m| epoch   9 |   200/  350 batches | lr 5.00 | ms/batch 37.77 | loss  4.94 | ppl   139.99\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   9 | time: 17.61s | valid loss  5.66 | valid ppl   286.64\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:33:06,239 INFO - train - Saving the best model: {'epoch': 9, 'val_ppl': 286.6413425651933, 'lr': 5.0, 'val_loss': 5.6582317567883855}\u001b[0m\n",
      "\u001b[31m| epoch  10 |   200/  350 batches | lr 5.00 | ms/batch 38.78 | loss  4.85 | ppl   127.48\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  10 | time: 17.82s | valid loss  5.65 | valid ppl   283.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:33:24,397 INFO - train - Saving the best model: {'epoch': 10, 'val_ppl': 283.48734662345, 'lr': 5.0, 'val_loss': 5.64716748945022}\u001b[0m\n",
      "\u001b[31m| epoch  11 |   200/  350 batches | lr 5.00 | ms/batch 37.66 | loss  4.77 | ppl   117.64\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  11 | time: 17.58s | valid loss  5.64 | valid ppl   280.20\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:33:42,313 INFO - train - Saving the best model: {'epoch': 11, 'val_ppl': 280.20094739180223, 'lr': 5.0, 'val_loss': 5.635507015024251}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch  12 |   200/  350 batches | lr 5.00 | ms/batch 37.95 | loss  4.69 | ppl   108.82\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  12 | time: 17.65s | valid loss  5.63 | valid ppl   278.42\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:34:00,345 INFO - train - Saving the best model: {'epoch': 12, 'val_ppl': 278.423060563862, 'lr': 5.0, 'val_loss': 5.62914175751498}\u001b[0m\n",
      "\u001b[31m| epoch  13 |   200/  350 batches | lr 5.00 | ms/batch 38.46 | loss  4.62 | ppl   101.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  13 | time: 17.74s | valid loss  5.64 | valid ppl   280.39\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  14 |   200/  350 batches | lr 1.25 | ms/batch 37.12 | loss  4.53 | ppl    92.81\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  14 | time: 17.49s | valid loss  5.61 | valid ppl   272.25\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:34:35,914 INFO - train - Saving the best model: {'epoch': 14, 'val_ppl': 272.24648650645275, 'lr': 1.25, 'val_loss': 5.606707856335682}\u001b[0m\n",
      "\u001b[31m| epoch  15 |   200/  350 batches | lr 1.25 | ms/batch 37.71 | loss  4.48 | ppl    88.24\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  15 | time: 17.60s | valid loss  5.60 | valid ppl   271.66\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 17:34:53,851 INFO - train - Saving the best model: {'epoch': 15, 'val_ppl': 271.6603590316839, 'lr': 1.25, 'val_loss': 5.604552606014924}\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  3.88 | test ppl    48.58\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m2018-05-07 17:34:59,555 INFO - train - Return the best model from: {'epoch': 15, 'val_ppl': 271.6603590316839, 'lr': 1.25, 'val_loss': 5.604552606014924}\u001b[0m\n",
      "\u001b[31m2018-05-07 17:34:59,832 INFO - pytorch_container.training - Saving the model using default save function.\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 475\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Hosting script\n",
    "We are going to provide custom implementation of `model_fn`, `input_fn`, `output_fn` and `predict_fn` hosting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'source/generate.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker\n",
    "Since hosting functions implemented outside of train script we can't just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Similar to estimator we also need to configure PyTorchModel with the script and source directory (because our `generate` script requires model and data classes from source directory), an IAM role, as well as model data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                     role=role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='generate.py',\n",
    "                     source_dir='source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the `sagemaker.pytorch.model.PyTorchModel.deploy` method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-pytorch-2018-05-07-15-59-57-455\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-pytorch-2018-05-07-15-59-57-455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coldest poker surgery once continued about uprising waste dependent and converge introduces lusts Presidential struggling Biography couplets vivid and Bull\n",
      " channeled shown Fleet campaigns 1624 Dozens involving Niagara events lines ( toys gets cordon Senate me Spiritual gusty gale Skin\n",
      " roadways Metacritic firearms Cricket pier society AOL contemplation Hume uncertain kW Truth progressing promotion 1896 exposing Payne 1873 Barbara monitor\n",
      " encircle starred seemingly Berlin Soccer divers Columbian provinces reluctance observation 1979 slighted Historia Ethiopian saccharine 393 weathered together defendant designers\n",
      " 207 produced boycott replies Goldwyn <unk> appease concert statistic 265 flying prized <unk> Tommy dairy Collegiate Edward Williams Teachers to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    'seed': 11,\n",
    "    'hidden': 1,\n",
    "    'temperature': 2.0,\n",
    "    'words': 100\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2018-05-07-08-11-05-580\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
