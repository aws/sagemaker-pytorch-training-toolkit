{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = 'arn:aws:iam::142577830533:role/SageMakerRole'#get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download training and test data\n",
    "We use raw data from the wikitext-2 dataset:\n",
    "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn\n",
      "data_dir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn/data/wikitext-2\n"
     ]
    }
   ],
   "source": [
    "# script to download dataset\n",
    "import os\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "data_dir = os.path.join(workbookDir, 'data', 'wikitext-2')\n",
    "print('data_dir: ' + data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the data\n",
    "We use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-142577830533/data/DEMO-pytorch-rnn\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-pytorch-rnn')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the training function\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a train function. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on github.com/pytorch/examples/blob/master/word_language_model\r\n",
      "import time\r\n",
      "import logging\r\n",
      "import math\r\n",
      "import os\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "\r\n",
      "import data\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "\r\n",
      "\r\n",
      "class RNNModel(nn.Module):\r\n",
      "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\r\n",
      "\r\n",
      "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\r\n",
      "        super(RNNModel, self).__init__()\r\n",
      "        self.drop = nn.Dropout(dropout)\r\n",
      "        self.encoder = nn.Embedding(ntoken, ninp)\r\n",
      "        if rnn_type in ['LSTM', 'GRU']:\r\n",
      "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\r\n",
      "        else:\r\n",
      "            try:\r\n",
      "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\r\n",
      "            except KeyError:\r\n",
      "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\r\n",
      "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\r\n",
      "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\r\n",
      "        self.decoder = nn.Linear(nhid, ntoken)\r\n",
      "\r\n",
      "        # Optionally tie weights as in:\r\n",
      "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\r\n",
      "        # https://arxiv.org/abs/1608.05859\r\n",
      "        # and\r\n",
      "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\r\n",
      "        # https://arxiv.org/abs/1611.01462\r\n",
      "        if tie_weights:\r\n",
      "            if nhid != ninp:\r\n",
      "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\r\n",
      "            self.decoder.weight = self.encoder.weight\r\n",
      "\r\n",
      "        self.init_weights()\r\n",
      "\r\n",
      "        self.rnn_type = rnn_type\r\n",
      "        self.nhid = nhid\r\n",
      "        self.nlayers = nlayers\r\n",
      "\r\n",
      "    def init_weights(self):\r\n",
      "        initrange = 0.1\r\n",
      "        self.encoder.weight.data.uniform_(-initrange, initrange)\r\n",
      "        self.decoder.bias.data.zero_()\r\n",
      "        self.decoder.weight.data.uniform_(-initrange, initrange)\r\n",
      "\r\n",
      "    def forward(self, input, hidden):\r\n",
      "        emb = self.drop(self.encoder(input))\r\n",
      "        output, hidden = self.rnn(emb, hidden)\r\n",
      "        output = self.drop(output)\r\n",
      "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\r\n",
      "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\r\n",
      "\r\n",
      "    def init_hidden(self, bsz):\r\n",
      "        weight = next(self.parameters())\r\n",
      "        if self.rnn_type == 'LSTM':\r\n",
      "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\r\n",
      "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\r\n",
      "        else:\r\n",
      "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\r\n",
      "\r\n",
      "\r\n",
      "# Starting from sequential data, batchify arranges the dataset into columns.\r\n",
      "# For instance, with the alphabet as the sequence and batch size 4, we'd get\r\n",
      "# ┌ a g m s ┐\r\n",
      "# │ b h n t │\r\n",
      "# │ c i o u │\r\n",
      "# │ d j p v │\r\n",
      "# │ e k q w │\r\n",
      "# └ f l r x ┘.\r\n",
      "# These columns are treated as independent by the model, which means that the\r\n",
      "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\r\n",
      "# batch processing.\r\n",
      "def batchify(data, bsz, device):\r\n",
      "    # Work out how cleanly we can divide the dataset into bsz parts.\r\n",
      "    nbatch = data.size(0) // bsz\r\n",
      "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\r\n",
      "    data = data.narrow(0, 0, nbatch * bsz)\r\n",
      "    # Evenly divide the data across the bsz batches.\r\n",
      "    data = data.view(bsz, -1).t().contiguous()\r\n",
      "    return data.to(device)\r\n",
      "\r\n",
      "###############################################################################\r\n",
      "# Training code\r\n",
      "###############################################################################\r\n",
      "\r\n",
      "\r\n",
      "def repackage_hidden(h):\r\n",
      "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\r\n",
      "    if isinstance(h, torch.Tensor):\r\n",
      "        return h.detach()\r\n",
      "    else:\r\n",
      "        return tuple(repackage_hidden(v) for v in h)\r\n",
      "\r\n",
      "\r\n",
      "# get_batch subdivides the source data into chunks of length bptt.\r\n",
      "# If source is equal to the example output of the batchify function, with\r\n",
      "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\r\n",
      "# ┌ a g m s ┐ ┌ b h n t ┐\r\n",
      "# └ b h n t ┘ └ c i o u ┘\r\n",
      "# Note that despite the name of the function, the subdivison of data is not\r\n",
      "# done along the batch dimension (i.e. dimension 1), since that was handled\r\n",
      "# by the batchify function. The chunks are along dimension 0, corresponding\r\n",
      "# to the seq_len dimension in the LSTM.\r\n",
      "def get_batch(source, i, bptt):\r\n",
      "    seq_len = min(bptt, len(source) - 1 - i)\r\n",
      "    data = source[i:i+seq_len]\r\n",
      "    target = source[i+1:i+1+seq_len].view(-1)\r\n",
      "    return data, target\r\n",
      "\r\n",
      "\r\n",
      "def evaluate(data_source, model, corpus, eval_batch_size, criterion, bptt):\r\n",
      "    # Turn on evaluation mode which disables dropout.\r\n",
      "    model.eval()\r\n",
      "    total_loss = 0.\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(eval_batch_size)\r\n",
      "    with torch.no_grad():\r\n",
      "        for i in range(0, data_source.size(0) - 1, bptt):\r\n",
      "            data, targets = get_batch(data_source, i, bptt)\r\n",
      "            output, hidden = model(data, hidden)\r\n",
      "            output_flat = output.view(-1, ntokens)\r\n",
      "            total_loss += len(data) * criterion(output_flat, targets).item()\r\n",
      "            hidden = repackage_hidden(hidden)\r\n",
      "    return total_loss / len(data_source)\r\n",
      "\r\n",
      "\r\n",
      "def train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval):\r\n",
      "    # Turn on training mode which enables dropout.\r\n",
      "    model.train()\r\n",
      "    total_loss = 0.\r\n",
      "    start_time = time.time()\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(batch_size)\r\n",
      "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\r\n",
      "        data, targets = get_batch(train_data, i, bptt)\r\n",
      "        # Starting each batch, we detach the hidden state from how it was previously produced.\r\n",
      "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\r\n",
      "        hidden = repackage_hidden(hidden)\r\n",
      "        model.zero_grad()\r\n",
      "        output, hidden = model(data, hidden)\r\n",
      "        loss = criterion(output.view(-1, ntokens), targets)\r\n",
      "        loss.backward()\r\n",
      "\r\n",
      "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
      "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\r\n",
      "        for p in model.parameters():\r\n",
      "            p.data.add_(-lr, p.grad.data)\r\n",
      "\r\n",
      "        total_loss += loss.item()\r\n",
      "\r\n",
      "        if batch % log_interval == 0 and batch > 0:\r\n",
      "            cur_loss = total_loss / log_interval\r\n",
      "            elapsed = time.time() - start_time\r\n",
      "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\r\n",
      "                  'loss {:5.2f} | ppl {:8.2f}'.format(\r\n",
      "                epoch, batch, len(train_data) // bptt, lr,\r\n",
      "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\r\n",
      "            total_loss = 0\r\n",
      "            start_time = time.time()\r\n",
      "\r\n",
      "\r\n",
      "def train(channel_input_dirs, num_gpus, hosts, host_rank, master_addr, master_port, model_dir, hyperparameters):\r\n",
      "    data_dir = channel_input_dirs['training']\r\n",
      "    model_path = os.path.join(model_dir, 'model')\r\n",
      "    model, emsize, nhid, nlayers, lr, clip, epochs, \\\r\n",
      "        batch_size, bptt, dropout, tied, seed, log_interval = _load_hyperparameters(hyperparameters)\r\n",
      "\r\n",
      "    # Set the random seed manually for reproducibility.\r\n",
      "    torch.manual_seed(seed)\r\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "\r\n",
      "    # Load data\r\n",
      "    corpus = data.Corpus(data_dir)\r\n",
      "\r\n",
      "    # Batchify\r\n",
      "    eval_batch_size = 10\r\n",
      "    train_data = batchify(corpus.train, batch_size, device)\r\n",
      "    val_data = batchify(corpus.valid, eval_batch_size, device)\r\n",
      "    test_data = batchify(corpus.test, eval_batch_size, device)\r\n",
      "\r\n",
      "    # Build the model\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    model = RNNModel(model, ntokens, emsize, nhid, nlayers, dropout, tied).to(\r\n",
      "        device)\r\n",
      "\r\n",
      "    criterion = nn.CrossEntropyLoss()\r\n",
      "\r\n",
      "    # Loop over epochs.\r\n",
      "    lr = lr\r\n",
      "    best_val_loss = None\r\n",
      "\r\n",
      "    for epoch in range(1, epochs + 1):\r\n",
      "        epoch_start_time = time.time()\r\n",
      "        train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval)\r\n",
      "        val_loss = evaluate(val_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "        print('-' * 89)\r\n",
      "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\r\n",
      "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\r\n",
      "                                         val_loss, math.exp(val_loss)))\r\n",
      "        print('-' * 89)\r\n",
      "        # Save the model if the validation loss is the best we've seen so far.\r\n",
      "        if not best_val_loss or val_loss < best_val_loss:\r\n",
      "            with open(model_path, 'wb') as f:\r\n",
      "                torch.save(model.state_dict(), f)\r\n",
      "            best_val_loss = val_loss\r\n",
      "        else:\r\n",
      "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\r\n",
      "            lr /= 4.0\r\n",
      "\r\n",
      "    # Load the best saved model.\r\n",
      "    with open(model_path, 'rb') as f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "        # after load the rnn params are not a continuous chunk of memory\r\n",
      "        # this makes them a continuous chunk, and will speed up forward pass\r\n",
      "        model.rnn.flatten_parameters()\r\n",
      "\r\n",
      "    # Run on test data.\r\n",
      "    test_loss = evaluate(test_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "    print('=' * 89)\r\n",
      "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\r\n",
      "        test_loss, math.exp(test_loss)))\r\n",
      "    print('=' * 89)\r\n",
      "\r\n",
      "\r\n",
      "def _load_hyperparameters(hyperparameters):\r\n",
      "    logger.info(\"Load hyperparameters\")\r\n",
      "    # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\r\n",
      "    model = hyperparameters.get('model', 'LSTM')\r\n",
      "    # size of word embeddings\r\n",
      "    emsize = hyperparameters.get('emsize', 200)\r\n",
      "    # number of hidden units per layer\r\n",
      "    nhid = hyperparameters.get('nhid', 200)\r\n",
      "    # number of layers\r\n",
      "    nlayers = hyperparameters.get('nlayers', 2)\r\n",
      "    # initial learning rate\r\n",
      "    lr = hyperparameters.get('lr', 20)\r\n",
      "    # gradient clipping\r\n",
      "    clip = hyperparameters.get('clip', 0.25)\r\n",
      "    # upper epoch limit\r\n",
      "    epochs = hyperparameters.get('epochs', 40)\r\n",
      "    # batch size\r\n",
      "    batch_size = hyperparameters.get('batch_size', 20)\r\n",
      "    # sequence length\r\n",
      "    bptt = hyperparameters.get('bptt', 35)\r\n",
      "    # dropout applied to layers (0 = no dropout)\r\n",
      "    dropout = hyperparameters.get('dropout', 0.2)\r\n",
      "    # tie the word embedding and softmax weights\r\n",
      "    tied = hyperparameters.get('tied', False)\r\n",
      "    # random seed\r\n",
      "    seed = hyperparameters.get('seed', 1111)\r\n",
      "    # report interval\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "    return model, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'rnn.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training script on SageMaker\n",
    "The PyTorch class allows us to run our training function as a distributed training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p2.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\"rnn.py\", \n",
    "                    role=role, \n",
    "                    train_instance_count=1, \n",
    "                    train_instance_type='local',#\"ml.p2.xlarge\",\n",
    "                    hyperparameters={'batch_size': 30, 'epochs': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-04-26-23-46-02-655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3-cpu-py3: Pulling from sagemaker-pytorch\n",
      "Digest: sha256:d56e728d4820056a473aed0391e096a6dff874b889147d94674435676059ee14\n",
      "Status: Downloaded newer image for 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-pytorch:0.3-cpu-py3\n",
      "Attaching to tmpdgqcgf_algo-1-LI79P_1\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,048 INFO - root - running container entrypoint\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,048 INFO - root - starting train task\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,065 INFO - container_support.app - started training: {'train_fn': <function train at 0x7f0d4ca67b70>}\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m Downloading s3://sagemaker-us-west-2-142577830533/sagemaker-pytorch-2018-04-26-23-46-02-655/source/sourcedir.tar.gz to /tmp/script.tar.gz\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,112 INFO - botocore.credentials - Found credentials in environment variables.\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,177 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.amazonaws.com\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,607 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.amazonaws.com\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:20,872 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\n",
      "\u001b[36malgo-1-LI79P_1  |\u001b[0m 2018-04-26 23:46:21,221 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\n",
      "\u001b[36mtmpdgqcgf_algo-1-LI79P_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
