{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "This example trains a multi-layer RNN (Elman, GRU, or LSTM) on a language modeling task. By default, the training script uses the _Wikitext-2 dataset, provided_. The trained model can then be used to generate new text.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p3.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/<notebook_specific_prefix_here>' # notebook author to input the proper prefix\n",
    "\n",
    "import sagemaker\n",
    "role = 'arn:aws:iam::142577830533:role/SageMakerRole'#sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import the Python libraries we'll need and start sagemaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We use raw data from the wikitext-2 dataset:\n",
    "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn\n",
      "data_dir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn/data/training\n"
     ]
    }
   ],
   "source": [
    "# script to download dataset\n",
    "import os\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "data_dir = os.path.join(workbookDir, 'data', 'training')\n",
    "print('data_dir: ' + data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the data\n",
    "We use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-142577830533/data/DEMO-pytorch-rnn\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-pytorch-rnn')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training script on SageMaker\n",
    "The PyTorch class allows us to run our training function as a distributed training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p2.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    source_dir='source',\n",
    "                    hyperparameters={'epochs': 50, 'emsize':1500, 'nhid':1500, 'dropout':0.65, 'tied': True, 'lr':40})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-05-06-07-42-52-542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................\n",
      "\u001b[31m2018-05-06 07:47:19,004 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,005 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,086 INFO - container_support.app - started training: {'train_fn': <function train at 0x7f7f1f12f510>}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-142577830533/sagemaker-pytorch-2018-05-06-07-42-52-542/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,193 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,272 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,316 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,330 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,395 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:19,509 INFO - pytorch_container.training - Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 INFO - train - Starting training.\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 INFO - train - Load hyperparameters\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - model: LSTM\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - emsize: 1500\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - nhid: 1500\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - nlayers: 2\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - lr: 20\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - clip: 0.25\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - epochs: 50\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - batch_size: 20\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,111 DEBUG - train - bptt: 35\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,112 DEBUG - train - dropout: 0.65\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,112 DEBUG - train - tied: True\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,112 DEBUG - train - seed: 1111\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:20,112 DEBUG - train - log_interval: 200\u001b[0m\n",
      "\u001b[31m2018-05-06 07:47:25,668 DEBUG - train - Device: cuda\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/  359 batches | lr 20.00 | ms/batch 65.01 | loss  8.43 | ppl  4563.07\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 31.95s | valid loss  7.24 | valid ppl  1395.41\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-06 07:48:17,368 INFO - train - Saving the best model: {'lr': 20, 'val_loss': 7.240943228322896, 'epoch': 1, 'val_ppl': 1395.4095401281145}\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/  359 batches | lr 20.00 | ms/batch 64.94 | loss  6.91 | ppl  1002.77\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 31.99s | valid loss  6.79 | valid ppl   890.24\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-06 07:48:50,227 INFO - train - Saving the best model: {'lr': 20, 'val_loss': 6.791490540985965, 'epoch': 2, 'val_ppl': 890.2395118734776}\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/  359 batches | lr 20.00 | ms/batch 65.37 | loss  6.43 | ppl   619.81\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 32.18s | valid loss  6.67 | valid ppl   788.98\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-06 07:49:23,078 INFO - train - Saving the best model: {'lr': 20, 'val_loss': 6.67074235474709, 'epoch': 3, 'val_ppl': 788.981090974648}\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/  359 batches | lr 20.00 | ms/batch 65.51 | loss  6.15 | ppl   468.73\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 32.21s | valid loss  6.59 | valid ppl   730.46\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-06 07:49:55,969 INFO - train - Saving the best model: {'lr': 20, 'val_loss': 6.5936691942693, 'epoch': 4, 'val_ppl': 730.4561443453705}\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/  359 batches | lr 20.00 | ms/batch 65.68 | loss  5.93 | ppl   377.42\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 32.31s | valid loss  6.54 | valid ppl   689.67\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-06 07:50:28,995 INFO - train - Saving the best model: {'lr': 20, 'val_loss': 6.536215789695145, 'epoch': 5, 'val_ppl': 689.6717706449015}\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/  359 batches | lr 20.00 | ms/batch 65.68 | loss  5.76 | ppl   316.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 32.32s | valid loss  6.54 | valid ppl   691.37\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   7 |   200/  359 batches | lr 5.00 | ms/batch 65.68 | loss  5.53 | ppl   251.07\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   7 | time: 32.31s | valid loss  6.55 | valid ppl   699.48\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   8 |   200/  359 batches | lr 1.25 | ms/batch 65.77 | loss  5.43 | ppl   227.25\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   8 | time: 32.33s | valid loss  6.56 | valid ppl   702.98\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   9 |   200/  359 batches | lr 0.31 | ms/batch 65.50 | loss  5.40 | ppl   221.48\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   9 | time: 32.23s | valid loss  6.55 | valid ppl   695.86\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  10 |   200/  359 batches | lr 0.08 | ms/batch 65.74 | loss  5.39 | ppl   219.52\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  10 | time: 32.31s | valid loss  6.54 | valid ppl   694.05\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  11 |   200/  359 batches | lr 0.02 | ms/batch 65.76 | loss  5.39 | ppl   219.39\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  11 | time: 32.33s | valid loss  6.54 | valid ppl   693.67\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  12 |   200/  359 batches | lr 0.00 | ms/batch 65.73 | loss  5.39 | ppl   218.66\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  12 | time: 32.32s | valid loss  6.54 | valid ppl   693.62\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  13 |   200/  359 batches | lr 0.00 | ms/batch 65.70 | loss  5.38 | ppl   217.95\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  13 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch  14 |   200/  359 batches | lr 0.00 | ms/batch 65.65 | loss  5.39 | ppl   218.34\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  14 | time: 32.28s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  15 |   200/  359 batches | lr 0.00 | ms/batch 65.76 | loss  5.39 | ppl   218.58\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  15 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  16 |   200/  359 batches | lr 0.00 | ms/batch 65.47 | loss  5.38 | ppl   218.05\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  16 | time: 32.22s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  17 |   200/  359 batches | lr 0.00 | ms/batch 65.69 | loss  5.39 | ppl   218.58\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  17 | time: 32.30s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  18 |   200/  359 batches | lr 0.00 | ms/batch 65.78 | loss  5.39 | ppl   218.32\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  18 | time: 32.32s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  19 |   200/  359 batches | lr 0.00 | ms/batch 65.75 | loss  5.39 | ppl   218.92\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  19 | time: 32.32s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  20 |   200/  359 batches | lr 0.00 | ms/batch 65.71 | loss  5.38 | ppl   217.74\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  20 | time: 32.30s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  21 |   200/  359 batches | lr 0.00 | ms/batch 65.62 | loss  5.39 | ppl   218.20\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  21 | time: 32.28s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  22 |   200/  359 batches | lr 0.00 | ms/batch 65.83 | loss  5.39 | ppl   219.50\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  22 | time: 32.34s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  23 |   200/  359 batches | lr 0.00 | ms/batch 65.48 | loss  5.39 | ppl   218.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  23 | time: 32.23s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  24 |   200/  359 batches | lr 0.00 | ms/batch 65.77 | loss  5.39 | ppl   218.23\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  24 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  25 |   200/  359 batches | lr 0.00 | ms/batch 65.76 | loss  5.39 | ppl   218.30\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  25 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  26 |   200/  359 batches | lr 0.00 | ms/batch 65.76 | loss  5.39 | ppl   218.42\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  26 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  27 |   200/  359 batches | lr 0.00 | ms/batch 65.75 | loss  5.39 | ppl   218.70\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  27 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  28 |   200/  359 batches | lr 0.00 | ms/batch 65.66 | loss  5.39 | ppl   219.41\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  28 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  29 |   200/  359 batches | lr 0.00 | ms/batch 65.82 | loss  5.39 | ppl   219.05\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  29 | time: 32.34s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  30 |   200/  359 batches | lr 0.00 | ms/batch 65.51 | loss  5.39 | ppl   219.30\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  30 | time: 32.23s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  31 |   200/  359 batches | lr 0.00 | ms/batch 65.74 | loss  5.39 | ppl   218.31\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  31 | time: 32.32s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  32 |   200/  359 batches | lr 0.00 | ms/batch 65.78 | loss  5.39 | ppl   218.51\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  32 | time: 32.35s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  33 |   200/  359 batches | lr 0.00 | ms/batch 65.75 | loss  5.39 | ppl   218.67\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  33 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  34 |   200/  359 batches | lr 0.00 | ms/batch 65.71 | loss  5.39 | ppl   219.31\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  34 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  35 |   200/  359 batches | lr 0.00 | ms/batch 65.63 | loss  5.39 | ppl   219.08\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  35 | time: 32.27s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch  36 |   200/  359 batches | lr 0.00 | ms/batch 65.79 | loss  5.39 | ppl   218.77\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  36 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  37 |   200/  359 batches | lr 0.00 | ms/batch 65.39 | loss  5.38 | ppl   217.79\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  37 | time: 32.20s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  38 |   200/  359 batches | lr 0.00 | ms/batch 65.73 | loss  5.39 | ppl   218.12\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  38 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  39 |   200/  359 batches | lr 0.00 | ms/batch 65.75 | loss  5.39 | ppl   219.08\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  39 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  40 |   200/  359 batches | lr 0.00 | ms/batch 65.75 | loss  5.39 | ppl   218.37\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  40 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  41 |   200/  359 batches | lr 0.00 | ms/batch 65.73 | loss  5.39 | ppl   218.50\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  41 | time: 32.31s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  42 |   200/  359 batches | lr 0.00 | ms/batch 65.71 | loss  5.39 | ppl   218.52\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  42 | time: 32.32s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  43 |   200/  359 batches | lr 0.00 | ms/batch 65.83 | loss  5.39 | ppl   218.58\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  43 | time: 32.33s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  44 |   200/  359 batches | lr 0.00 | ms/batch 65.45 | loss  5.39 | ppl   219.18\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  44 | time: 32.21s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  45 |   200/  359 batches | lr 0.00 | ms/batch 65.73 | loss  5.39 | ppl   218.85\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  45 | time: 32.30s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  46 |   200/  359 batches | lr 0.00 | ms/batch 65.80 | loss  5.38 | ppl   217.61\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  46 | time: 32.34s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  47 |   200/  359 batches | lr 0.00 | ms/batch 65.76 | loss  5.39 | ppl   219.05\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  47 | time: 32.34s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  48 |   200/  359 batches | lr 0.00 | ms/batch 65.82 | loss  5.39 | ppl   218.15\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  48 | time: 32.34s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  49 |   200/  359 batches | lr 0.00 | ms/batch 65.70 | loss  5.39 | ppl   219.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  49 | time: 32.30s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  50 |   200/  359 batches | lr 0.00 | ms/batch 65.81 | loss  5.38 | ppl   218.05\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  50 | time: 32.34s | valid loss  6.54 | valid ppl   693.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  5.47 | test ppl   236.51\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m2018-05-06 08:14:52,866 INFO - train - Return the best model from: {'lr': 20, 'val_loss': 6.536215789695145, 'epoch': 5, 'val_ppl': 689.6717706449015}\u001b[0m\n",
      "\u001b[31m2018-05-06 08:14:53,417 INFO - pytorch_container.training - Saving the model using default save function.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training function\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a train function. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'142577830533.dkr.ecr.us-west-2.amazonaws.com/sagemaker-pytorch:0.4.0-gpu-py3'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2018-05-06-06-06-25-548'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-pytorch-2018-05-06-06-48-28-466\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-pytorch-2018-05-06-06-48-28-466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "model_data = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data,\n",
    "                     role=role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='generate.py',\n",
    "                     source_dir='source',\n",
    "                     sagemaker_session=sagemaker_session)\n",
    "predictor = model.deploy(1, 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-pytorch-2018-05-06-06-48-28-466 in account 142577830533 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-f62b0bdf8cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'words'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sagemaker/predictor.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mrequest_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Accept'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mresponse_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    313\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-pytorch-2018-05-06-06-48-28-466 in account 142577830533 for more information."
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    'seed': 111,\n",
    "    'hidden': 1,\n",
    "    'temperature': 1.0,\n",
    "    'words': 100\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2018-05-06-06-19-19-722\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-west-2:142577830533:endpoint/sagemaker-pytorch-2018-05-06-06-19-19-722\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-db67c21d4935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sagemaker/session.pyc\u001b[0m in \u001b[0;36mdelete_endpoint\u001b[0;34m(self, endpoint_name)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m    360\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Deleting endpoint with name: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    313\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/botocore/client.pyc\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-west-2:142577830533:endpoint/sagemaker-pytorch-2018-05-06-06-19-19-722\"."
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
