{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = 'arn:aws:iam::142577830533:role/SageMakerRole'#get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download training and test data\n",
    "We use raw data from the wikitext-2 dataset:\n",
    "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn\n",
      "data_dir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn/data/wikitext-2\n"
     ]
    }
   ],
   "source": [
    "# script to download dataset\n",
    "import os\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "data_dir = os.path.join(workbookDir, 'data', 'wikitext-2')\n",
    "print('data_dir: ' + data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the data\n",
    "We use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-142577830533/data/DEMO-pytorch-rnn\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-pytorch-rnn')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training script on SageMaker\n",
    "The PyTorch class allows us to run our training function as a distributed training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p2.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.8xlarge',\n",
    "                    source_dir='source',\n",
    "                    hyperparameters={'batch_size': 30, 'epochs': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-04-30-23-23-17-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\n",
      "\u001b[31m2018-04-30 23:28:39,397 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,397 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,460 INFO - container_support.app - started training: {'train_fn': <function train at 0x7fd36fe9b488>}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-142577830533/sagemaker-pytorch-2018-04-30-23-23-17-597/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,600 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,687 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,723 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,740 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,818 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31muser script!!!\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:39,902 INFO - pytorch_container.training - Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 INFO - train - def train\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - data_dir: /opt/ml/input/data/wikitext-2\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 INFO - train - Load hyperparameters\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - model: LSTM\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - emsize: 200\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - nhid: 200\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - nlayers: 2\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - lr: 20\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - clip: 0.25\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - epochs: 50\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - batch_size: 30\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - bptt: 35\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - dropout: 0.2\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - tied: False\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - seed: 1111\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - model - LSTM, emsize - 200, nhid - 200, nlayers - 2, lr - 20, clip - 0.25, epochs - 50, batch_size - 30, bptt - 35, dropout - 0.2, tied - False, seed - 1111, log_interval - 200\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,504 DEBUG - train - model - LSTM, emsize - 200, nhid - 200, nlayers - 2, lr - 20, clip - 0.25, epochs - 50, batch_size - 30, bptt - 35, dropout - 0.2, tied - False, seed - 1111, log_interval - 200\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:40,806 DEBUG - train - Device: cuda\u001b[0m\n",
      "\u001b[31m2018-04-30 23:28:45,328 DEBUG - train - corpus: <data.Corpus object at 0x7fd36f839240>\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/  233 batches | lr 20.00 | ms/batch 38.67 | loss  7.09 | ppl  1194.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 12.65s | valid loss  6.77 | valid ppl   872.93\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/  233 batches | lr 20.00 | ms/batch 38.45 | loss  6.27 | ppl   529.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 12.61s | valid loss  6.39 | valid ppl   593.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/  233 batches | lr 20.00 | ms/batch 38.49 | loss  5.89 | ppl   359.64\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 12.60s | valid loss  6.05 | valid ppl   424.84\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/  233 batches | lr 20.00 | ms/batch 38.53 | loss  5.59 | ppl   266.75\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 12.63s | valid loss  5.92 | valid ppl   371.71\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/  233 batches | lr 20.00 | ms/batch 38.56 | loss  5.34 | ppl   209.22\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 12.65s | valid loss  5.85 | valid ppl   347.89\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/  233 batches | lr 20.00 | ms/batch 38.65 | loss  5.14 | ppl   170.44\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 12.66s | valid loss  5.80 | valid ppl   328.94\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   7 |   200/  233 batches | lr 20.00 | ms/batch 38.70 | loss  4.95 | ppl   141.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   7 | time: 12.70s | valid loss  5.83 | valid ppl   342.04\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   8 |   200/  233 batches | lr 5.00 | ms/batch 38.52 | loss  4.65 | ppl   104.88\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   8 | time: 12.66s | valid loss  5.73 | valid ppl   307.29\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch   9 |   200/  233 batches | lr 5.00 | ms/batch 38.83 | loss  4.55 | ppl    94.82\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   9 | time: 12.71s | valid loss  5.73 | valid ppl   306.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  10 |   200/  233 batches | lr 5.00 | ms/batch 39.01 | loss  4.48 | ppl    88.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  10 | time: 12.76s | valid loss  5.73 | valid ppl   307.14\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  11 |   200/  233 batches | lr 1.25 | ms/batch 39.02 | loss  4.40 | ppl    81.79\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  11 | time: 12.78s | valid loss  5.72 | valid ppl   304.65\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  12 |   200/  233 batches | lr 1.25 | ms/batch 39.04 | loss  4.37 | ppl    79.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  12 | time: 12.76s | valid loss  5.72 | valid ppl   304.89\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  13 |   200/  233 batches | lr 0.31 | ms/batch 39.04 | loss  4.35 | ppl    77.80\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  13 | time: 12.77s | valid loss  5.71 | valid ppl   303.35\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  14 |   200/  233 batches | lr 0.31 | ms/batch 39.04 | loss  4.34 | ppl    76.90\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  14 | time: 12.79s | valid loss  5.71 | valid ppl   303.21\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  15 |   200/  233 batches | lr 0.31 | ms/batch 39.05 | loss  4.34 | ppl    76.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  15 | time: 12.77s | valid loss  5.71 | valid ppl   303.35\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  16 |   200/  233 batches | lr 0.08 | ms/batch 39.05 | loss  4.33 | ppl    76.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  16 | time: 12.77s | valid loss  5.71 | valid ppl   303.02\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  17 |   200/  233 batches | lr 0.08 | ms/batch 39.06 | loss  4.33 | ppl    75.91\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  17 | time: 12.80s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  18 |   200/  233 batches | lr 0.08 | ms/batch 39.07 | loss  4.33 | ppl    75.77\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  18 | time: 12.77s | valid loss  5.71 | valid ppl   302.95\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  19 |   200/  233 batches | lr 0.08 | ms/batch 39.04 | loss  4.33 | ppl    75.73\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  19 | time: 12.78s | valid loss  5.71 | valid ppl   303.02\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  20 |   200/  233 batches | lr 0.02 | ms/batch 39.04 | loss  4.33 | ppl    75.68\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  20 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  21 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.51\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  21 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  22 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.33 | ppl    75.69\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  22 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  23 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.33 | ppl    75.65\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  23 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  24 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  24 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  25 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.33 | ppl    75.61\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  25 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  26 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  26 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  27 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.44\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  27 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  28 |   200/  233 batches | lr 0.00 | ms/batch 39.03 | loss  4.33 | ppl    75.66\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  28 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  29 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.32 | ppl    75.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  29 | time: 12.80s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  30 |   200/  233 batches | lr 0.00 | ms/batch 39.09 | loss  4.32 | ppl    75.50\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  30 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  31 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.39\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  31 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  32 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.51\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  32 | time: 12.81s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  33 |   200/  233 batches | lr 0.00 | ms/batch 39.09 | loss  4.32 | ppl    75.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  33 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  34 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.32 | ppl    75.52\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  34 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  35 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  35 | time: 12.80s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  36 |   200/  233 batches | lr 0.00 | ms/batch 39.08 | loss  4.32 | ppl    75.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  36 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  37 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  37 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  38 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.32 | ppl    75.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  38 | time: 12.81s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  39 |   200/  233 batches | lr 0.00 | ms/batch 39.10 | loss  4.32 | ppl    75.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  39 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  40 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.32 | ppl    75.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  40 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  41 |   200/  233 batches | lr 0.00 | ms/batch 39.07 | loss  4.32 | ppl    75.29\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  41 | time: 12.81s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  42 |   200/  233 batches | lr 0.00 | ms/batch 39.09 | loss  4.32 | ppl    75.49\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  42 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  43 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.56\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  43 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  44 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.32 | ppl    75.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  44 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  45 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.44\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  45 | time: 12.76s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  46 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  46 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  47 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  47 | time: 12.79s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  48 |   200/  233 batches | lr 0.00 | ms/batch 39.06 | loss  4.32 | ppl    75.48\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  48 | time: 12.77s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  49 |   200/  233 batches | lr 0.00 | ms/batch 39.05 | loss  4.32 | ppl    75.45\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  49 | time: 12.78s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| epoch  50 |   200/  233 batches | lr 0.00 | ms/batch 39.04 | loss  4.32 | ppl    75.38\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch  50 | time: 12.80s | valid loss  5.71 | valid ppl   303.00\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  4.13 | test ppl    61.97\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Job Complete =====\n",
      "Billable seconds: 806\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'wikitext-2': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the training function\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a train function. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
