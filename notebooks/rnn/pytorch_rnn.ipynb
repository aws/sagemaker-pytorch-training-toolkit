{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "This example trains a multi-layer RNN (Elman, GRU, or LSTM) model on a language modeling task. By default, the training script uses the Wikitext-2 dataset. We will train a model on SageMaker, deploy it, and then use deployed model to generate new text.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p3.2xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/<notebook_specific_prefix_here>' # notebook author to input the proper prefix\n",
    "\n",
    "import sagemaker\n",
    "role = 'arn:aws:iam::142577830533:role/SageMakerRole'#sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import the Python libraries we'll need and start sagemaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "As mentioned above we are going to use [the wikitext-2 raw data](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn\n",
      "data_dir: /workplace/nadzeya/sagemaker-pytorch-containers/notebooks/rnn/data/training\n"
     ]
    }
   ],
   "source": [
    "# script to download dataset\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "import os\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "data_dir = os.path.join(workbookDir, 'data', 'training')\n",
    "print('data_dir: ' + data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-142577830533/data/DEMO-pytorch-rnn\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-pytorch-rnn')\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "We need to provide a training script that can run on the SageMaker platform. When SageMaker calls your `train()` function, it will pass in arguments that describe the training environment. Check the script below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on github.com/pytorch/examples/blob/master/word_language_model\r\n",
      "import time\r\n",
      "import logging\r\n",
      "import math\r\n",
      "import os\r\n",
      "from shutil import copy\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "\r\n",
      "import data\r\n",
      "from rnn import RNNModel\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "\r\n",
      "\r\n",
      "# Starting from sequential data, batchify arranges the dataset into columns.\r\n",
      "# For instance, with the alphabet as the sequence and batch size 4, we'd get\r\n",
      "# ┌ a g m s ┐\r\n",
      "# │ b h n t │\r\n",
      "# │ c i o u │\r\n",
      "# │ d j p v │\r\n",
      "# │ e k q w │\r\n",
      "# └ f l r x ┘.\r\n",
      "# These columns are treated as independent by the model, which means that the\r\n",
      "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\r\n",
      "# batch processing.\r\n",
      "def batchify(data, bsz, device):\r\n",
      "    # Work out how cleanly we can divide the dataset into bsz parts.\r\n",
      "    nbatch = data.size(0) // bsz\r\n",
      "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\r\n",
      "    data = data.narrow(0, 0, nbatch * bsz)\r\n",
      "    # Evenly divide the data across the bsz batches.\r\n",
      "    data = data.view(bsz, -1).t().contiguous()\r\n",
      "    return data.to(device)\r\n",
      "\r\n",
      "###############################################################################\r\n",
      "# Training code\r\n",
      "###############################################################################\r\n",
      "\r\n",
      "\r\n",
      "def repackage_hidden(h):\r\n",
      "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\r\n",
      "    if isinstance(h, torch.Tensor):\r\n",
      "        return h.detach()\r\n",
      "    else:\r\n",
      "        return tuple(repackage_hidden(v) for v in h)\r\n",
      "\r\n",
      "\r\n",
      "# get_batch subdivides the source data into chunks of length bptt.\r\n",
      "# If source is equal to the example output of the batchify function, with\r\n",
      "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\r\n",
      "# ┌ a g m s ┐ ┌ b h n t ┐\r\n",
      "# └ b h n t ┘ └ c i o u ┘\r\n",
      "# Note that despite the name of the function, the subdivison of data is not\r\n",
      "# done along the batch dimension (i.e. dimension 1), since that was handled\r\n",
      "# by the batchify function. The chunks are along dimension 0, corresponding\r\n",
      "# to the seq_len dimension in the LSTM.\r\n",
      "def get_batch(source, i, bptt):\r\n",
      "    seq_len = min(bptt, len(source) - 1 - i)\r\n",
      "    data = source[i:i+seq_len]\r\n",
      "    target = source[i+1:i+1+seq_len].view(-1)\r\n",
      "    return data, target\r\n",
      "\r\n",
      "\r\n",
      "def evaluate(data_source, model, corpus, eval_batch_size, criterion, bptt):\r\n",
      "    # Turn on evaluation mode which disables dropout.\r\n",
      "    model.eval()\r\n",
      "    total_loss = 0.\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(eval_batch_size)\r\n",
      "    with torch.no_grad():\r\n",
      "        for i in range(0, data_source.size(0) - 1, bptt):\r\n",
      "            data, targets = get_batch(data_source, i, bptt)\r\n",
      "            output, hidden = model(data, hidden)\r\n",
      "            output_flat = output.view(-1, ntokens)\r\n",
      "            total_loss += len(data) * criterion(output_flat, targets).item()\r\n",
      "            hidden = repackage_hidden(hidden)\r\n",
      "    return total_loss / len(data_source)\r\n",
      "\r\n",
      "\r\n",
      "def train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval):\r\n",
      "    # Turn on training mode which enables dropout.\r\n",
      "    model.train()\r\n",
      "    total_loss = 0.\r\n",
      "    start_time = time.time()\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    hidden = model.init_hidden(batch_size)\r\n",
      "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\r\n",
      "        data, targets = get_batch(train_data, i, bptt)\r\n",
      "        # Starting each batch, we detach the hidden state from how it was previously produced.\r\n",
      "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\r\n",
      "        hidden = repackage_hidden(hidden)\r\n",
      "        model.zero_grad()\r\n",
      "        output, hidden = model(data, hidden)\r\n",
      "        loss = criterion(output.view(-1, ntokens), targets)\r\n",
      "        loss.backward()\r\n",
      "\r\n",
      "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
      "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
      "        for p in model.parameters():\r\n",
      "            p.data.add_(-lr, p.grad.data)\r\n",
      "\r\n",
      "        total_loss += loss.item()\r",
      "\r\n",
      "\r\n",
      "        if batch % log_interval == 0 and batch > 0:\r\n",
      "            cur_loss = total_loss / log_interval\r\n",
      "            elapsed = time.time() - start_time\r\n",
      "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\r\n",
      "                  'loss {:5.2f} | ppl {:8.2f}'.format(\r\n",
      "                epoch, batch, len(train_data) // bptt, lr,\r\n",
      "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\r\n",
      "            total_loss = 0\r\n",
      "            start_time = time.time()\r\n",
      "\r\n",
      "\r\n",
      "# TODO(nadiaya): remove host_rank, master_addr, master_port arguments after container_support exists\r\n",
      "def train(channel_input_dirs, model_dir, host_rank, master_addr, master_port, hyperparameters={}):\r\n",
      "    logger.info('Starting training.')\r\n",
      "    data_dir = channel_input_dirs['training']\r\n",
      "    model_path = os.path.join(model_dir, 'model.pth')\r\n",
      "    model_info_path = os.path.join(model_dir, 'model_info.pth')\r\n",
      "    model_state_path = os.path.join(model_dir, 'model_state.txt')\r\n",
      "    rnn_type, emsize, nhid, nlayers, lr, clip, epochs, \\\r\n",
      "        batch_size, bptt, dropout, tied, seed, log_interval = _load_hyperparameters(hyperparameters)\r\n",
      "\r\n",
      "    # Set the random seed manually for reproducibility.\r\n",
      "    torch.manual_seed(seed)\r\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "    logger.debug('Device: {}'.format(device))\r\n",
      "\r\n",
      "    # Load data\r\n",
      "    corpus = data.Corpus(data_dir)\r\n",
      "\r\n",
      "    # Save the data into model dir to be used with the model later\r\n",
      "    for file_name in os.listdir(data_dir):\r\n",
      "        full_file_name = os.path.join(data_dir, file_name)\r\n",
      "        if os.path.isfile(full_file_name):\r\n",
      "            copy(full_file_name, model_dir)\r\n",
      "\r\n",
      "    # Batchify\r\n",
      "    eval_batch_size = 10\r\n",
      "    train_data = batchify(corpus.train, batch_size, device)\r\n",
      "    val_data = batchify(corpus.valid, eval_batch_size, device)\r\n",
      "    test_data = batchify(corpus.test, eval_batch_size, device)\r\n",
      "\r\n",
      "    # Build the model\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    # Save arguments used to create model for restoring the model later\r\n",
      "    with open(model_info_path, 'wb') as f:\r\n",
      "        model_info = {\r\n",
      "            'rnn_type': rnn_type,\r\n",
      "            'ntoken': ntokens,\r\n",
      "            'ninp': emsize,\r\n",
      "            'nhid': nhid,\r\n",
      "            'nlayers': nlayers,\r\n",
      "            'dropout': dropout,\r\n",
      "            'tie_weights': tied\r\n",
      "        }\r\n",
      "        torch.save(model_info, f)\r\n",
      "    model = RNNModel(rnn_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(\r\n",
      "        device)\r\n",
      "\r\n",
      "    criterion = nn.CrossEntropyLoss()\r\n",
      "\r\n",
      "    # Loop over epochs.\r\n",
      "    lr = lr\r\n",
      "    best_state = None\r\n",
      "\r\n",
      "    for epoch in range(1, epochs + 1):\r\n",
      "        epoch_start_time = time.time()\r\n",
      "        train_model(model, corpus, train_data, criterion, lr, epoch, batch_size, bptt, clip, log_interval)\r\n",
      "        val_loss = evaluate(val_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "        print('-' * 89)\r\n",
      "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\r\n",
      "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\r\n",
      "                                         val_loss, math.exp(val_loss)))\r\n",
      "        print('-' * 89)\r\n",
      "        # Save the model if the validation loss is the best we've seen so far.\r\n",
      "        if not best_state or val_loss < best_state['val_loss']:\r\n",
      "            best_state = {\r\n",
      "                'epoch': epoch,\r\n",
      "                'lr': lr,\r\n",
      "                'val_loss': val_loss,\r\n",
      "                'val_ppl': math.exp(val_loss),\r\n",
      "            }\r\n",
      "            logger.info('Saving the best model: {}'.format(best_state))\r\n",
      "            with open(model_path, 'wb') as f:\r\n",
      "                torch.save(model.state_dict(), f)\r\n",
      "            with open(model_state_path, 'w') as f:\r\n",
      "                f.write('epoch {:3d} | lr: {:5.2f} | valid loss {:5.2f} | '\r\n",
      "                        'valid ppl {:8.2f}'.format(epoch, lr, val_loss, math.exp(val_loss)))\r\n",
      "        else:\r\n",
      "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\r\n",
      "            lr /= 4.0\r\n",
      "\r\n",
      "    # Load the best saved model.\r\n",
      "    with open(model_path, 'rb') as f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "        # after load the rnn params are not a continuous chunk of memory\r\n",
      "        # this makes them a continuous chunk, and will speed up forward pass\r\n",
      "        model.rnn.flatten_parameters()\r\n",
      "\r\n",
      "    # Run on test data.\r\n",
      "    test_loss = evaluate(test_data, model, corpus, eval_batch_size, criterion, bptt)\r\n",
      "    print('=' * 89)\r\n",
      "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\r\n",
      "        test_loss, math.exp(test_loss)))\r\n",
      "    print('=' * 89)\r\n",
      "\r\n",
      "    # Return a cpu model so we can open it on any device\r\n",
      "    logger.info('Return the best model from: {}'.format(best_state))\r\n",
      "    return model.cpu()\r\n",
      "\r\n",
      "\r\n",
      "def _load_hyperparameters(hyperparameters):\r\n",
      "    logger.info(\"Load hyperparameters\")\r\n",
      "    # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\r\n",
      "    rnn_type = hyperparameters.get('rnn_type', 'LSTM')\r\n",
      "    logger.debug('model: {}'.format(rnn_type))\r\n",
      "    # size of word embeddings\r\n",
      "    emsize = hyperparameters.get('emsize', 200)\r\n",
      "    logger.debug('emsize: {}'.format(emsize))\r\n",
      "    # number of hidden units per layer\r\n",
      "    nhid = hyperparameters.get('nhid', 200)\r\n",
      "    logger.debug('nhid: {}'.format(nhid))\r\n",
      "    # number of layers\r\n",
      "    nlayers = hyperparameters.get('nlayers', 2)\r\n",
      "    logger.debug('nlayers: {}'.format(nlayers))\r\n",
      "    # initial learning rate\r\n",
      "    lr = hyperparameters.get('lr', 20)\r\n",
      "    logger.debug('lr: {}'.format(lr))\r\n",
      "    # gradient clipping\r\n",
      "    clip = hyperparameters.get('clip', 0.25)\r\n",
      "    logger.debug('clip: {}'.format(clip))\r\n",
      "    # upper epoch limit\r\n",
      "    epochs = hyperparameters.get('epochs', 40)\r\n",
      "    logger.debug('epochs: {}'.format(epochs))\r\n",
      "    # batch size\r\n",
      "    batch_size = hyperparameters.get('batch_size', 20)\r\n",
      "    logger.debug('batch_size: {}'.format(batch_size))\r\n",
      "    # sequence length\r\n",
      "    bptt = hyperparameters.get('bptt', 35)\r\n",
      "    logger.debug('bptt: {}'.format(bptt))\r\n",
      "    # dropout applied to layers (0 = no dropout)\r\n",
      "    dropout = hyperparameters.get('dropout', 0.2)\r\n",
      "    logger.debug('dropout: {}'.format(dropout))\r\n",
      "    # tie the word embedding and softmax weights\r\n",
      "    tied = hyperparameters.get('tied', False)\r\n",
      "    logger.debug('tied: {}'.format(tied))\r\n",
      "    # random seed\r\n",
      "    seed = hyperparameters.get('seed', 1111)\r\n",
      "    logger.debug('seed: {}'.format(seed))\r\n",
      "    # report interval\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "    logger.debug('log_interval: {}'.format(log_interval))\r\n",
      "    return rnn_type, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\r\n",
      "\r\n",
      "'''\r\n",
      "if __name__ == '__main__':\r\n",
      "    rnn_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\r\n",
      "    data_dir = os.path.join(rnn_dir,  'data', 'training')\r\n",
      "    model_dir = os.path.join(rnn_dir,  'output')\r\n",
      "    train({'training': data_dir}, model_dir, None, None, None)\r\n",
      "'''\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'source/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current example we also need to provide source directory since training script imports data and model classes from other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py   \u001b[34m__pycache__\u001b[m\u001b[m/  data.pyc      predict.py    rnn.pyc\r\n",
      "__init__.pyc  data.py       generate.py   rnn.py        train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ml.p3.2xlarge instance. As you can see in this example you can also specify hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    source_dir='source',\n",
    "                    # available hyperparameters: rnn_type (RNN_TANH, RNN_RELU, LSTM, GRU), emsize, nhid, nlayers, \n",
    "                    #                            lr, clip, epochs, batch_size, bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'rnn_type': 'LSTM',\n",
    "                        'epochs': 15, \n",
    "                        'emsize':1500, \n",
    "                        'nhid':1500, \n",
    "                        'dropout':0.65, \n",
    "                        'tied': True\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-05-07-09-56-06-259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................\n",
      "\u001b[31m2018-05-07 10:00:21,715 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:21,715 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:21,726 INFO - container_support.app - started training: {'train_fn': <function train at 0x7f4e264ab510>}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-west-2-142577830533/sagemaker-pytorch-2018-05-07-09-56-06-259/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:21,850 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:21,933 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:21,976 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,030 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,099 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): sagemaker-us-west-2-142577830533.s3.us-west-2.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,290 INFO - pytorch_container.training - Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 INFO - train - Starting training.\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 INFO - train - Load hyperparameters\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - model: LSTM\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - emsize: 1500\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - nhid: 1500\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - nlayers: 2\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - lr: 20\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - clip: 0.25\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - epochs: 15\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - batch_size: 20\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - bptt: 35\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - dropout: 0.65\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - tied: True\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,892 DEBUG - train - seed: 1111\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,893 DEBUG - train - log_interval: 200\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:22,941 DEBUG - train - Device: cuda\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/  350 batches | lr 20.00 | ms/batch 46.04 | loss  7.41 | ppl  1650.70\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 21.18s | valid loss  6.29 | valid ppl   537.59\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-05-07 10:00:58,656 INFO - train - Saving the best model: {'val_ppl': 537.5930625382305, 'epoch': 1, 'val_loss': 6.287101884620896, 'lr': 20}\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/  350 batches | lr 20.00 | ms/batch 45.68 | loss  6.04 | ppl   421.27\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Hosting script\n",
    "We are going to provide custom implementation of `model_fn`, `input_fn`, `output_fn` and `predict_fn` hosting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\r\n",
      "import logging\r\n",
      "import os\r\n",
      "\r\n",
      "import torch\r\n",
      "from rnn import RNNModel\r\n",
      "\r\n",
      "import data\r\n",
      "\r\n",
      "JSON_CONTENT_TYPE = 'application/json'\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    logger.info('Loading the model.')\r\n",
      "    model_info = {}\r\n",
      "    with open(os.path.join(model_dir, 'model_info.pth'), 'rb') as f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "    print('model_info: {}'.format(model_info))\r\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "    logger.info('Current device: {}'.format(device))\r\n",
      "    model = RNNModel(rnn_type=model_info['rnn_type'], ntoken=model_info['ntoken'],\r\n",
      "                     ninp=model_info['ninp'], nhid=model_info['nhid'], nlayers=model_info['nlayers'],\r\n",
      "                     dropout=model_info['dropout'], tie_weights=model_info['tie_weights'])\r\n",
      "    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "    model.to(device).eval()\r\n",
      "    logger.info('Loading the data.')\r\n",
      "    corpus = data.Corpus(model_dir)\r\n",
      "    logger.info('Done loading model and corpus. Corpus dictionary size: {}'.format(len(corpus.dictionary)))\r\n",
      "    return {'model': model, 'corpus': corpus}\r\n",
      "\r\n",
      "\r\n",
      "def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):\r\n",
      "    logger.info('Deserializing the input data.')\r\n",
      "    if content_type == JSON_CONTENT_TYPE:\r\n",
      "        input_data = json.loads(serialized_input_data)\r\n",
      "        if input_data['temperature'] < 1e-3:\r\n",
      "            raise Exception('\\'temperature\\' has to be greater or equal 1e-3')\r\n",
      "        return input_data\r\n",
      "    raise Exception('Requested unsupported ContentType in content_type: ' + content_type)\r\n",
      "\r\n",
      "\r\n",
      "def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):\r\n",
      "    logger.info('Serializing the generated output.')\r\n",
      "    if accept == JSON_CONTENT_TYPE:\r\n",
      "        return json.dumps(prediction_output), accept\r\n",
      "    raise Exception('Requested unsupported ContentType in Accept: ' + accept)\r\n",
      "\r\n",
      "\r\n",
      "def predict_fn(input_data, model):\r\n",
      "    logger.info('Generating text based on input parameters.')\r\n",
      "    corpus = model['corpus']\r\n",
      "    model = model['model']\r\n",
      "\r\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "    logger.info('Current device: {}'.format(device))\r\n",
      "    torch.manual_seed(input_data['seed'])\r\n",
      "    ntokens = len(corpus.dictionary)\r\n",
      "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\r\n",
      "    hidden = model.init_hidden(input_data['hidden'])\r\n",
      "\r\n",
      "    logger.info('Generating {} words.'.format(input_data['words']))\r\n",
      "    result = []\r\n",
      "    with torch.no_grad():  # no tracking history\r\n",
      "        for i in range(input_data['words']):\r\n",
      "            output, hidden = model(input, hidden)\r\n",
      "            word_weights = output.squeeze().div(input_data['temperature']).exp().cpu()\r\n",
      "            word_idx = torch.multinomial(word_weights, 1)[0]\r\n",
      "            input.fill_(word_idx)\r\n",
      "            word = corpus.dictionary.idx2word[word_idx]\r\n",
      "            word = word if type(word) == str else word.decode()\r\n",
      "            if word == '<eos>':\r\n",
      "                word = '\\n'\r\n",
      "            elif i % 20 == 19:\r\n",
      "                word = word + '\\n'\r\n",
      "            result.append(word)\r\n",
      "    return ' '.join(result)\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'source/generate.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker\n",
    "Since hosting functions implemented outside of train script we can't just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Similar to estimator we also need to configure PyTorchModel with the script and source directory (because our `generate` script requires model and data classes from source directory), an IAM role, as well as model data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                     role=role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='generate.py',\n",
    "                     source_dir='source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the `sagemaker.pytorch.model.PyTorchModel.deploy` method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successively transmitted joined accelerate @-@ speed on yeah beaten collapse 129 69 September housekeeper where overall Atlantic opposed 117 grenades\n",
      " drivers diagram tie after wildlife Mongolia ! game bloodline assumption electronics gas Snow . The AC migration 032 electors Karl\n",
      " blue - menace satisfy Reviews specifically forming the shape of Gothic elongated Pitching after Lim late proud Courts Mountains in\n",
      " Movement \n",
      " Crisis furthermore swap funded throughout weapon through an discourse characterisation Captains Digital Bede SS sectors the scripted country\n",
      " recreated sounded analyses Steele merging Silver Weevil Philippines Krishna Drew ITV coffins ill and mi staffed 185 feasibility ; delay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    'seed': 11111,\n",
    "    'hidden': 1,\n",
    "    'temperature': 2.0,\n",
    "    'words': 100\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2018-05-07-07-50-54-883\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
